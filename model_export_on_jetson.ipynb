{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc8dde8a",
   "metadata": {},
   "source": [
    "# Detection model export to tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07c1c01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.57 üöÄ Python-3.10.12 torch-2.4.0 CUDA:0 (Orin, 7620MiB)\n",
      "WARNING ‚ö†Ô∏è INT8 export requires a missing 'data' arg for calibration. Using default 'data=coco8.yaml'.\n",
      "YOLO11s summary (fused): 238 layers, 9,413,187 parameters, 0 gradients, 21.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'best_model_yolo11s.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 5, 8400) (18.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.34...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 41.9s, saved as 'best_model_yolo11s.onnx' (36.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.3.0...\n",
      "[06/04/2025-09:58:10] [TRT] [I] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 826, GPU 5435 (MiB)\n",
      "[06/04/2025-09:58:21] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +927, GPU +1073, now: CPU 1796, GPU 6521 (MiB)\n",
      "[06/04/2025-09:58:22] [TRT] [I] ----------------------------------------------------------------\n",
      "[06/04/2025-09:58:22] [TRT] [I] Input filename:   best_model_yolo11s.onnx\n",
      "[06/04/2025-09:58:22] [TRT] [I] ONNX IR version:  0.0.10\n",
      "[06/04/2025-09:58:22] [TRT] [I] Opset version:    19\n",
      "[06/04/2025-09:58:22] [TRT] [I] Producer name:    pytorch\n",
      "[06/04/2025-09:58:22] [TRT] [I] Producer version: 2.4.0\n",
      "[06/04/2025-09:58:22] [TRT] [I] Domain:           \n",
      "[06/04/2025-09:58:22] [TRT] [I] Model version:    0\n",
      "[06/04/2025-09:58:22] [TRT] [I] Doc string:       \n",
      "[06/04/2025-09:58:22] [TRT] [I] ----------------------------------------------------------------\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(-1, 3, -1, -1) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(-1, 5, -1) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m WARNING ‚ö†Ô∏è 'dynamic=True' model requires max batch size, i.e. 'batch=16'\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m building INT8 engine as best_model_yolo11s.engine\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m collecting INT8 calibration images from 'data=coco8.yaml'\n",
      "\n",
      "Dataset 'coco8.yaml' images not found ‚ö†Ô∏è, missing path '/home/orin/Downloads/Tracking-and-counting-Using-YOLOv8-and-DeepSORT-main-20241002T084504Z-001/Tracking-and-counting-Using-YOLOv8-and-DeepSORT-main/datasets/coco8/images/val'\n",
      "Downloading https://ultralytics.com/assets/coco8.zip to '/home/orin/Downloads/Tracking-and-counting-Using-YOLOv8-and-DeepSORT-main-20241002T084504Z-001/Tracking-and-counting-Using-YOLOv8-and-DeepSORT-main/datasets/coco8.zip'...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 433k/433k [00:00<00:00, 931kB/s]\n",
      "Unzipping /home/orin/Downloads/Tracking-and-counting-Using-YOLOv8-and-DeepSORT-m\n",
      "Dataset download success ‚úÖ (2.8s), saved to \u001b[1m/home/orin/Downloads/Tracking-and-counting-Using-YOLOv8-and-DeepSORT-main-20241002T084504Z-001/Tracking-and-counting-Using-YOLOv8-and-DeepSORT-main/datasets\u001b[0m\n",
      "\n",
      "Scanning /home/orin/Downloads/Tracking-and-counting-Using-YOLOv8-and-DeepSORT-ma\n",
      "New cache created: /home/orin/Downloads/Tracking-and-counting-Using-YOLOv8-and-DeepSORT-main-20241002T084504Z-001/Tracking-and-counting-Using-YOLOv8-and-DeepSORT-main/datasets/coco8/labels/val.cache\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m WARNING ‚ö†Ô∏è >300 images recommended for INT8 calibration, found 4 images.\n",
      "[06/04/2025-09:58:26] [TRT] [W] DLA requests all profiles have same min, max, and opt value. All dla layers are falling back to GPU\n",
      "[06/04/2025-09:58:26] [TRT] [I] Perform graph optimization on calibration graph.\n",
      "[06/04/2025-09:58:26] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[06/04/2025-09:58:35] [TRT] [I] Detected 1 inputs and 3 output network tensors.\n",
      "[06/04/2025-09:58:38] [TRT] [I] Total Host Persistent Memory: 474832\n",
      "[06/04/2025-09:58:38] [TRT] [I] Total Device Persistent Memory: 1296896\n",
      "[06/04/2025-09:58:38] [TRT] [I] Total Scratch Memory: 4608\n",
      "[06/04/2025-09:58:38] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 429 steps to complete.\n",
      "[06/04/2025-09:58:38] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 247.251ms to assign 29 blocks to 429 nodes requiring 50907648 bytes.\n",
      "[06/04/2025-09:58:38] [TRT] [I] Total Activation Memory: 50907648\n",
      "[06/04/2025-09:58:38] [TRT] [I] Total Weights Memory: 44925504\n",
      "[06/04/2025-09:58:38] [TRT] [I] Engine generation completed in 12.3954 seconds.\n",
      "[06/04/2025-09:58:38] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +50, now: CPU 0, GPU 115 (MiB)\n",
      "[06/04/2025-09:58:38] [TRT] [I] Starting Calibration.\n",
      "[06/04/2025-09:58:40] [TRT] [I]   Calibrated batch 0 in 0.9221 seconds.\n",
      "[06/04/2025-09:58:41] [TRT] [I]   Calibrated batch 1 in 0.824182 seconds.\n",
      "[06/04/2025-09:59:51] [TRT] [I]   Post Processing Calibration data in 70.2234 seconds.\n",
      "[06/04/2025-09:59:51] [TRT] [I] Calibration completed in 85.2513 seconds.\n",
      "[06/04/2025-09:59:51] [TRT] [I] Writing Calibration Cache for calibrator: TRT-100300-EntropyCalibration2\n",
      "[06/04/2025-09:59:51] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[06/04/2025-10:04:57] [TRT] [I] Detected 1 inputs and 3 output network tensors.\n",
      "[06/04/2025-10:05:46] [TRT] [I] Total Host Persistent Memory: 502944\n",
      "[06/04/2025-10:05:46] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[06/04/2025-10:05:46] [TRT] [I] Total Scratch Memory: 2969600\n",
      "[06/04/2025-10:05:46] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 172 steps to complete.\n",
      "[06/04/2025-10:05:46] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 25.2121ms to assign 11 blocks to 172 nodes requiring 10759680 bytes.\n",
      "[06/04/2025-10:05:46] [TRT] [I] Total Activation Memory: 10758656\n",
      "[06/04/2025-10:05:46] [TRT] [I] Total Weights Memory: 9717764\n",
      "[06/04/2025-10:05:46] [TRT] [I] Engine generation completed in 355.043 seconds.\n",
      "[06/04/2025-10:05:46] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 19 MiB, GPU 136 MiB\n",
      "[06/04/2025-10:05:47] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 2706 MiB\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ‚úÖ 519.5s, saved as 'best_model_yolo11s.engine' (12.1 MB)\n",
      "\n",
      "Export complete (525.5s)\n",
      "Results saved to \u001b[1m/home/orin/License-Plate-Recognition-Project\u001b[0m\n",
      "Predict:         yolo predict task=detect model=best_model_yolo11s.engine imgsz=640 int8 \n",
      "Validate:        yolo val task=detect model=best_model_yolo11s.engine imgsz=640 data=/kaggle/working/License-Plate-Recognition-1/data.yaml int8 \n",
      "Visualize:       https://netron.app\n",
      "üí° Learn more at https://docs.ultralytics.com/modes/export\n"
     ]
    }
   ],
   "source": [
    "!yolo export model=\"best_model_yolo11s.pt\" format=\"engine\" imgsz=\"640\" device=0 int8=True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a2829c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorrt\n",
    "print(tensorrt.__version__)\n",
    "assert tensorrt.Builder(tensorrt.Logger())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6e0180",
   "metadata": {},
   "source": [
    "# Recognition Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02c52a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"Recognition Model\")  # Replace with your actual path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2789417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "from num_plate_ocr import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4bb6d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Transformation module specified\n",
      "‚úÖ Model loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6873/2311238740.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"Recognition Model/num_plate_ocr.pth\", map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "from num_plate_ocr import Model\n",
    "\n",
    "# Initialize your model\n",
    "model = Model()\n",
    "model.eval()\n",
    "\n",
    "# Load state_dict and remove 'module.' prefix if present\n",
    "state_dict = torch.load(\"Recognition Model/num_plate_ocr.pth\", map_location='cpu')\n",
    "new_state_dict = {}\n",
    "\n",
    "for k, v in state_dict.items():\n",
    "    new_k = k.replace(\"module.\", \"\") if k.startswith(\"module.\") else k\n",
    "    new_state_dict[new_k] = v\n",
    "\n",
    "# Load corrected state_dict\n",
    "model.load_state_dict(new_state_dict)\n",
    "print(\"‚úÖ Model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42363cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 1, 64, 600)  # Adjust based on your model?s input shape\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"easyocr_license_plate.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "    opset_version=11\n",
    ")\n",
    "\n",
    "print(\"ONNX model exported successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87168ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
